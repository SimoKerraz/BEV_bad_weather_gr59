# Bird eye view for autonomous cars under adverse weather conditions

# Abstract 
Advanced automotive active-safety systems, in general, and autonomous vehicles, in
particular, rely heavily on visual data to classify and localize objects such as pedestri-
ans, traffic signs and lights, and other nearby cars, to assist the corresponding vehicles
maneuver safely in their environments. However, the performance of object detection
methods could degrade rather significantly under challenging weather scenarios includ-
ing rainy conditions. Our goal includes surveying and analyzing the performance of
BEV methods trained and tested in the paper X in which visual data is captured under
very often clear and sometimes rainy conditions. Moreover, we survey and evaluate
the efficacy and limitations of the model used in X.

# Nuscenes Exploration
The model in the paper was trained on the Nuscenes data set. In this section, we want to
explore the potency of extreme weather conditions in this data set. Out of all the adverse
weather conditions, we shall mainly focus on 3 main ones : fog , snow and rain. We can
mention that although this dataset might contain some visuals captured in rainy conditions,
they do not have weather tagging like other datasets such as BDD100k. The Nuscenes
dataset is comprised of high-definition sensor data collected from a diverse set of urban
driving scenarios in multiple cities worldwide. 

![alt text](https://github.com/SimoKerraz/BEV_bad_weather_gr59/blob/main/afterRain.png)

We can see that rain data represents 20% of the whole dataset, whereas fog and snow
camera pictures are nonexistent in the dataset. This leads us to speculate that a model
trained on the Nuscenes dataset will not be able to generalize to cases of rainy conditions,
and will definitely perform poorly on fog and snow datasets. It is also worth noting that
several images in the data set are wrongly tagged as rainy weather when they actually show
clear or cloudy conditions.
 
![alt text](https://github.com/SimoKerraz/BEV_bad_weather_gr59/blob/main/Rplot.jpeg)


Please refer to DLAV.pdf for an extensive motivation and overview exploration on the data. 

# Data download
In order to get the mini Nuscenes data set : 
```
mkdir -p /data/nuscenes  # Make the directory to store the nuScenes dataset in.

wget https://www.nuscenes.org/data/v1.0-mini.tgz  # Download the nuScenes mini split.

tar -xf v1.0-mini.tgz -C /data/sets/nuscenes  # Uncompress the nuScenes mini split.

pip install nuscenes-devkit &> /dev/null  # Install nuScenes.

```
 

# Setup 
In order to get the code running, Python 3.7 should be used. The following packages are needed also : 

```
pytorch
numpy
pickle
pyquaternion
shapely
lmdb
OpenCV 3.2.0
PyClipper 1.0.6
```

You will need then to clone the repository of the original github of the paper : 


```
cd /translating-images-into-maps 
git clone https://github.com/avishkarsaha/translating-images-into-maps.git

```




# Pretrained model

Pretrained models and their configs required to load/train them can be downloaded from here:
```
https://www.icloud.com/iclouddrive/0aaSjW59DEqgUDKyy1uw0iSVg#nuscenes%5Fdata

```
# Training 
 
To train the model we do as follows : 
```
python train1.py

```
# Validation 

In orther to validate the model : 
```
python validate1.py

```


# Contribution 
## Overview 

In a first step, the existing and pre-trained model presented in the paper X should be tested
on the Nuscenes dataset and the stated accuracies verified and confirmed. The next step
would be filtering out the data collected in rainy (and foggy and snowy) conditions, to com-
pare the performance of the model in those situations. Moreover, the entire dataset, or at
least a part of the data collected in good weather conditions, augmented with generated
droplets on the images would be tested and evaluated on the original model; in the aim to
analyze the data augmentation method and itâ€™s impact on the performance. Here again,
the two evaluations on the original dataset, with and without filtering out good weather
conditions, serves as a comparison metric to understand the effectiveness of the data aug-
mentation.

The second step would be to re-train the given model, using the augmented dataset which
should heavily represent adverse weather conditions. The resulting model should again be
evaluated with the original dataset, with and without filtering out good weather conditions

## Data Augmentation

We use the [Rain Rendering for Evaluating and Improving Robustness to Bad Weather](https://arxiv.org/abs/2009.03683) paper to augment clear weather images with controllable amount of rain. 

![alt text](https://github.com/SimoKerraz/BEV_bad_weather_gr59/blob/main/Rendering.png)

For that we will need to 

 ### Setup
Create your conda virtual environment:

```
conda create --name py36_weatheraugment python=3.6 opencv numpy matplotlib tqdm imageio pillow natsort glob2 scipy scikit-learn scikit-image pexpect -y

conda activate py36_weatheraugment

pip install pyclipper imutils

```

### Running the code
The renderer augment sequences of images with rain, using the following required data:

* images
* depth maps
* calibration files (optional, KITTI format)
 
The structure has to be as follows : 
```
data/source/DATASET/SEQUENCE/rgb/file0001.png           # Source images (color, 8 bits)
data/source/DATASET/SEQUENCE/depth/file0001.png         # Depth images (16 bits, with depth_in_meter = depth/256.)
```

Upon success, the renderer will output:
```
data/output/DATASET/SEQUENCE/rain/10mm/rainy_image/file0001.png     # Rainy images (here, 10mm/hr rain)
data/output/DATASET/SEQUENCE/rain/10mm/rainy_mask/file0001.png      # Rainy masks (int32 showing rain drops opacity, useful for rain detection/removal works) 
data/output/DATASET/SEQUENCE/envmap/file0001.png       

```

We then clone the following github : 

```
git clone https://github.com/astra-vision/rain-rendering.git
```

### Running

Once data and config are prepared, you may run the rendering with:

```
python main.py --dataset nuscene --intensity 25 --frame_end 10 

```

Output will be located in data/output/nuscene

## Training the model
Now that we have augmented our data, we will need to add the current data to the original one in the same folder and structure and retrain the model. 

To do that : 

 * We can either do it locally : 
    ```
 
    python train1.py 
    
    ```
 * On scitas using : 
    ```
    sbatch Trainscript
 
    ```
    
Please refer to DLAV.pdf for the metrics and the findings




